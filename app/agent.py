from langchain_community.llms import LlamaCpp
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
import os

from app.config import CHROMA_DB_PATH, PHI2_MODEL_PATH, LLAMA3B_MODEL_PATH
from app.spl_engine import SPLEngine

# Initialize SPL Engine
spl_engine = SPLEngine()

# Load the LLM model (using LlamaCpp for GGUF)
# Ensure you have either phi-2.gguf or llama-3b.gguf in the models/ directory
try:
    # Choose the model you want to use
    model_path = LLAMA3B_MODEL_PATH
    
    llm = LlamaCpp(
        model_path=model_path,
        n_ctx=2048, # Context window size
        n_gpu_layers=-1, # Offload all layers to GPU if available
        verbose=True, # Enable verbose output for debugging
    )
except Exception as e:
    print(f"Error loading LLM model for RAG: {e}")
    llm = None

# Load the embeddings model
try:
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
except Exception as e:
    print(f"Error loading embeddings model for RAG: {e}")
    embeddings = None

# Load the Chroma vector store
try:
    vectorstore = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
except Exception as e:
    print(f"Error loading Chroma vector store: {e}")
    vectorstore = None
    retriever = None

def get_rag_response(query: str) -> str:
    """
    Generates a response using a simple RAG flow:
    1. Retrieve relevant documents from Chroma
    2. Inject context into the prompt
    3. Ask the LLM to answer
    """
    if llm is None or retriever is None:
        return "RAG system not initialized. Cannot generate context-aware reply."
    try:
        # Get relevant documents from the retriever
        docs = retriever.invoke(query)

        context = "\n\n".join([d.page_content for d in docs])

        prompt = f"""
You are a restaurant voice assistant.

Answer ONLY the user's question.
Do NOT include unrelated information.
Be concise and specific.
If the answer is not present, say you don't know.

Context:
{context}

User question:
{query}

Answer (1â€“2 sentences max):
"""


        result = llm.generate([prompt])

        generation = result.generations[0][0]
        text = generation.text
        info = generation.generation_info or {}

        print("\nðŸ“Š LLM TOKEN USAGE")
        print("Prompt tokens:", info.get("prompt_tokens"))
        print("Completion tokens:", info.get("completion_tokens"))
        print("Total tokens:", info.get("total_tokens"))

        return text
    except Exception as e:
        return f"Error generating RAG response: {e}"
